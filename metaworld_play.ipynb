{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a notebook to check that metaworld is working, and a testbed for new RL algos \n",
    "import metaworld\n",
    "\n",
    "SEED = 0  # some seed number here\n",
    "benchmark = metaworld.ML1('pick-place-v2', seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assembly-v2', 'basketball-v2', 'bin-picking-v2', 'box-close-v2', 'button-press-topdown-v2', 'button-press-topdown-wall-v2', 'button-press-v2', 'button-press-wall-v2', 'coffee-button-v2', 'coffee-pull-v2', 'coffee-push-v2', 'dial-turn-v2', 'disassemble-v2', 'door-close-v2', 'door-lock-v2', 'door-open-v2', 'door-unlock-v2', 'hand-insert-v2', 'drawer-close-v2', 'drawer-open-v2', 'faucet-open-v2', 'faucet-close-v2', 'hammer-v2', 'handle-press-side-v2', 'handle-press-v2', 'handle-pull-side-v2', 'handle-pull-v2', 'lever-pull-v2', 'peg-insert-side-v2', 'pick-place-wall-v2', 'pick-out-of-hole-v2', 'reach-v2', 'push-back-v2', 'push-v2', 'pick-place-v2', 'plate-slide-v2', 'plate-slide-side-v2', 'plate-slide-back-v2', 'plate-slide-back-side-v2', 'peg-unplug-side-v2', 'soccer-v2', 'stick-push-v2', 'stick-pull-v2', 'push-wall-v2', 'reach-wall-v2', 'shelf-place-v2', 'sweep-into-v2', 'sweep-v2', 'window-open-v2', 'window-close-v2']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(metaworld.ML1.ENV_NAMES)  # Check out the available environments\n",
    "\n",
    "ml1 = metaworld.ML1('pick-place-v2') # Construct the benchmark, sampling tasks\n",
    "\n",
    "env = ml1.train_classes['pick-place-v2']()  # Create an environment with task `pick_place`\n",
    "task = random.choice(ml1.train_tasks)\n",
    "env.set_task(task)  # Set task\n",
    "\n",
    "obs, info = env.reset()  # Reset environment\n",
    "a = env.action_space.sample()  # Sample an action\n",
    "obs, reward, terminal, truncated, info = env.step(a)  # Step the environment with the sampled random action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml10 = metaworld.ML10() # Construct the benchmark, sampling tasks\n",
    "\n",
    "training_envs = []\n",
    "for name, env_cls in ml10.train_classes.items():\n",
    "  env = env_cls()\n",
    "  task = random.choice([task for task in ml10.train_tasks\n",
    "                        if task.env_name == name])\n",
    "  env.set_task(task)\n",
    "  training_envs.append(env)\n",
    "\n",
    "for env in training_envs:\n",
    "  obs, info = env.reset()  # Reset environment\n",
    "  a = env.action_space.sample()  # Sample an action\n",
    "  obs, reward, terminated, truncated, info = env.step(a)  # Step the environment with the sampled random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets try continual_world\n",
    "# run single\n",
    "\n",
    "from continualworld.envs import get_single_env, get_cl_env\n",
    "from continualworld.utils.utils import get_activation_from_str\n",
    "from continualworld.sac.models import MlpActor\n",
    "from continualworld.utils.run_utils import get_sac_class\n",
    "\n",
    "steps_per_task = 1000000\n",
    "activation=\"lrelu\"\n",
    "num_tasks=1\n",
    "hidden_sizes=[256, 256, 256, 256]\n",
    "tasks=['basketball-v2']\n",
    "# logger = EpochLogger(args[\"logger_output\"], config=args, group_id=args[\"group_id\"])\n",
    "\n",
    "logger = None\n",
    "steps = steps_per_task * len(tasks)\n",
    "num_heads = num_tasks\n",
    "train_env = get_cl_env(tasks, steps_per_task)\n",
    "test_envs = [\n",
    "    get_single_env(task, one_hot_idx=i, one_hot_len=num_tasks) for i, task in enumerate(tasks)\n",
    "]\n",
    "\n",
    "actor_kwargs = dict(\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    activation=get_activation_from_str(activation),\n",
    "    use_layer_norm=True,\n",
    "    num_heads=num_heads,\n",
    "    hide_task_id=True,\n",
    ")\n",
    "critic_kwargs = dict(\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    activation=get_activation_from_str(activation),\n",
    "    use_layer_norm=True,\n",
    "    num_heads=num_heads,\n",
    "    hide_task_id=True,\n",
    ")\n",
    "\n",
    "actor_cl = MlpActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from continualworld.utils.enums import BufferType\n",
    "lr=0.0025\n",
    "alpha='auto'\n",
    "buffer_type=\"fifo\"\n",
    "gamma=0.99\n",
    "seed=13\n",
    "clipnorm=None\n",
    "target_output_std = 0.089\n",
    "agent_policy_exploration = False\n",
    "cl_reg_coef=0.0  # strength of MAS/EWC\n",
    "regularize_critic = False\n",
    "\n",
    "vanilla_sac_kwargs = {\n",
    "    \"env\": train_env,\n",
    "    \"test_envs\": test_envs,\n",
    "    \"logger\": logger,\n",
    "    \"seed\": seed,\n",
    "    \"steps\": steps,\n",
    "    \"log_every\": 100,\n",
    "    \"replay_size\": 10000,\n",
    "    \"batch_size\": 10,\n",
    "    \"actor_cl\": actor_cl,\n",
    "    \"actor_kwargs\": actor_kwargs,\n",
    "    \"critic_kwargs\": critic_kwargs,\n",
    "    \"buffer_type\": BufferType(buffer_type),\n",
    "    \"reset_buffer_on_task_change\": True,\n",
    "    \"reset_optimizer_on_task_change\": True,\n",
    "    \"lr\": lr,\n",
    "    \"alpha\": alpha,\n",
    "    \"reset_critic_on_task_change\": True,\n",
    "    \"clipnorm\": clipnorm,\n",
    "    \"gamma\": gamma,\n",
    "    \"target_output_std\": target_output_std,\n",
    "    \"agent_policy_exploration\": agent_policy_exploration,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_method = None  # 'mas'\n",
    "\n",
    "sac_class = get_sac_class(cl_method)\n",
    "if cl_method is None:\n",
    "    sac = sac_class(**vanilla_sac_kwargs)\n",
    "elif cl_method in [\"l2\", \"ewc\", \"mas\"]:\n",
    "    sac = sac_class(\n",
    "        **vanilla_sac_kwargs, \n",
    "        cl_reg_coef=cl_reg_coef, \n",
    "        regularize_critic=regularize_critic\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/continual_world/continualworld/sac/sac.py:535\u001b[0m, in \u001b[0;36mSAC.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A method to run the SAC training, after the object has been created.\"\"\"\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 535\u001b[0m obs, episode_return, episode_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# Main loop: collect experience in env and update/log each epoch\u001b[39;00m\n\u001b[1;32m    538\u001b[0m current_task_timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/code/continual_world/continualworld/envs.py:145\u001b[0m, in \u001b[0;36mContinualLearningEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_steps_bound()\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur_seq_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/continual_world/continualworld/utils/wrappers.py:33\u001b[0m, in \u001b[0;36mSuccessCounter.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/wrappers/time_limit.py:68\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/continual_world/continualworld/utils/wrappers.py:68\u001b[0m, in \u001b[0;36mOneHotAdder.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_one_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/continual_world/continualworld/utils/wrappers.py:61\u001b[0m, in \u001b[0;36mOneHotAdder._append_one_hot\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_one_hot_dim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_one_hot_dim]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_append\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "sac.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continual_world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
